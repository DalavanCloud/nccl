<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" type="text/css" href="doc.css">
</head>
<body>

<div class="main">
<a name="top"></a>

<h1>NCCL User Manual</h1>

<a name="#1"></a><h2>1 Purpose</h2>

<p>NCCL is a communication library targetted at CUDA devices. It is optimized to
perform inter-device data movement and reductions.

<p>NCCL is able to work on single nodes as well as multiple nodes, using a
combination of NVLink, PCI point-to-point, shared memory, Infiniband or Sockets.
It features automatic topology detection to optimize bandwidth on platforms
combining multiple technologies.

<p>Five collective operations are currently implemented : allreduce, reduce,
broadcast, reduce_scatter and allgather. The API is similar to equivalent MPI
operations.

<a name="#2"></a><h2>2 Operation</h2>

<a name="#3.1"></a><h3>3.1 Communicators</h3>

<p>NCCL defines operations which exchange data between a group of CUDA devices.
A group is called "communicator", similarly to the notion of MPI communicator,
and the creation of a communicator is the first step needed before launching any
communication operation.

<p>When creating a communicator, a rank between <i>0</i> and <i>n-1</i> has
to be assigned to each of the <i>n</i> CUDA devices which are part of the
communicator.

<p>Given that static mapping of ranks to CUDA devices, the ncclCommInitRank and
ncclCommInitAll functions will create <i>n</i> communicator objects,
each communicator object being associated to a fixed rank. Those objects will
then be used to launch communication operations.

<a name="#3.2"></a><h3>3.2 Collective communication primitives</h3>

<a name="#3.3.1"></a><h4>3.3.1 Usage</h4>
<p>Like MPI collective operations, NCCL collective operations have to be called
for each rank (hence CUDA device) to form a complete collective operation.
Failure to do so will result in other ranks waiting indefinitely.

<a name="#3.3.2"></a><h4>3.3.2 All-reduce (ncclAllReduce)</h4>
<p>The AllReduce operation is performing reductions on data (e.g. sum, max)
across devices and writing the result in the receive buffers of every rank.
<p><center><img src="AllReduce.png" width=300px></center>

<a name="#3.3.3"></a><h4>3.3.3 Reduce (ncclReduce)</h4>
<p>The Reduce operation is performing the same operation as AllReduce, but
writes the result only in the receive buffers of a specified <i>root</i> rank.
<p><center><img src="Reduce.png" width=300px></center>

<a name="#3.3.4"></a><h4>3.3.4 Broadcast (ncclBcast)</h4>
<p>The Broadcast operation is replicating data from a specified <i>root</i> rank
to all the other ranks.
<p><center><img src="Bcast.png" width=300px></center>
<p>Note : Reduce+Bcast is equivalent to AllReduce.

<a name="#3.3.5"></a><h4>3.3.5 Reduce-scatter (ncclReduceScatter)</h4>
<p>The ReduceScatter operation is performing the same operation as the Reduce
operation, except the result is scattered in equal blocks among ranks, each rank
getting a chunk of data based on its rank number.
<p><center><img src="ReduceScatter.png" width=300px></center>

<a name="#3.3.6"></a><h4>3.3.6 All-gather (ncclAllGather)</h4>
<p>The AllGather operation is exchanging data between ranks so that all ranks
get aggregated data from all ranks, in the order of the ranks numbers.
<p><center><img src="ReduceScatter.png" width=300px></center>
<p>Note: : ReduceScatter+AllGather is equivalent to AllReduce.

<a name="#3.3.7"></a><h4>3.3.7 Impact of rank/device mappings</h4>
<p>The AllReduce operation is rank-agnostic. Any reordering of the ranks will
not affect the outcome of the operations. 
<p>Reduce and Broadcast have a <i>root</i> argument, which is one of the ranks,
and is therefore impacted by a different rank/device mapping.
<p> ReduceScatter and AllGather are also affected, since the ranks determine the
data layout.
<a name="#3.3"></a><h3>3.3 Multiple devices, threads and processes</h3>
<p>Using multiple CUDA devices in an application can be done in different ways.
Applications can use a single thread to launch CUDA operations on the different
devices, use one thread for each device, use one process for each device, a
combination of all or even more complex systems.
<p>All these cases should be supported by NCCL : every process can own any
number of ranks within a group and the associated communicator objects can be
used from any thread of the process.
<a name="#3.4"></a><h3>3.4 Communicator creation</h3>
<p>To create a communicator, one needs to first create a unique object which will
be used by all processes and threads to synchronize and understand they are part
of the same communicator. This is done by calling ncclGetUniqueId.
<p>ncclGetUniqueId will return an <i>ID</i> which has to be broadcasted to all
participating processes using any CPU communication system, e.g. passing
the <i>ID</i> pointer to multiple threads, or broadcasting it to other processes
using MPI or another parallel environment like sockets.
<p>Then, the application can call ncclCommInitRank for every rank.
<p>The usage of ncclCommInitRank is similar to any collective call : it
has to be called exactly <i>n</i> times, with each call specifying a different
rank going from <i>0</i> to <i>n-1</i>.
<p>Important note : before calling <i>ncclCommInitRank</i>,
the application has to set the CUDA device which will be associated to the
specified <i>rank</i>.
<p>Another function, ncclCommInitAll, is provided as a convenience
function to create <i>n</i> communicator objects at once within a single
process. As it is limited to a single process, this function does not permit
inter-node communication. ncclCommInitAll is equivalent to calling a
combination of ncclGetUniqueId and ncclCommInitRank.
<pre>
ncclResult_t ncclCommInitAll(ncclComm_t* comm, int ndev, const int* devlist) {
  ncclUniqueId Id;
  ncclGetUniqueId(&amp;Id);
  ncclGroupStart();
  for (int i=0; i&lt;ndev; i++) {
    cudaSetDevice(devlist[i]);
    ncclCommInitRank(comm+i, ndev, Id, i);
  }
  ncclGroupEnd();
}  
</pre>
<p>Note : this is a simplified implementation of ncclCommInitAll, see the
ncclCommInitAll documentation for more information.
<a name="#3.5"></a><h3>3.5 CUDA Stream semantics</h3>
<p>NCCL calls are associated to a stream, passed as last argument of the
collective communication function. The NCCL call returns when the operation has
been effectively enqueued to the given stream, or returns an error. The
collective operations is then executed asynchronously on the CUDA device. The
operation status can be queried using standard CUDA semantics, e.g. calling
cudaStreamSynchronize or using CUDA events.
<a name="#3.6"></a><h3>3.6 Group calls</h3>
<p>When a single thread is managing multiple devices, group semantics must be
used.
<p>This is because every NCCL call may have to block, waiting for other
threads/ranks to arrive, before effectively posting the NCCL operation on the
given stream. Hence, a simple loop on multiple devices like shown below could
block on the first call waiting for the other ones :
<pre>
   for (int i=0; i&lt;nLocalDevs; i++) {
     ncclAllReduce(..., comm[i], stream[i];
   }
</pre>
<p>To define that these calls are part of the same collectives, ncclGroupStart
and ncclGroupEnd should be used :
<pre>
   ncclGroupStart();
   for (int i=0; i&lt;nLocalDevs; i++) {
     ncclAllReduce(..., comm[i], stream[i];
   }
   ncclGroupEnd();
</pre>
<p>This will tell NCCL to treat all calls between ncclGroupStart and
ncclGroupEnd as a single call to many devices. When called inside a group,
ncclAllReduce can return without having enqueued the operation on the stream.
Stream operations like cudaStreamSynchronize can therefore be called only after
ncclGroupEnd returns.
<p>Note : contrary to NCCL 1.x, there is no need to set the CUDA device before
every NCCL communication call within a group, but it is still needed when
calling ncclCommInitRank within a group.
<a name="#3.7"></a><h3>3.7 Difference with MPI</h3>
<p>The NCCL API and usage is similar to MPI but there are many minor
differences.
<a name="#3.8.1"></a><h4>3.8.1 Ranks/process mapping</h4>
<p>Similarly to the concept of MPI endpoints, NCCL does not require ranks to be
mapped 1:1 to MPI ranks. A NCCL communicator may have many ranks associated
to a single process (hence MPI rank if used with MPI).
<a name="#3.8.2"></a><h4>3.8.2 Reduce Scatter</h4>
<p>The ncclReduceScatter operation is similar to the MPI_Reduce_scatter_block
operation, not MPI_Reduce_scatter. The MPI_Reduce_scatter function is
intrinsically a "vector" function, and should have been called
MPI_Reduce_scatterv, while MPI_Reduce_scatter_block (defined later to fill the
missing semantics) is providing regular counts similarly to the mirror function
MPI_Allgather. This is an odditiy of MPI which has not been fixed for legitimate
retro-compatibility reasons and that NCCL does not follow.
<a name="#3.8.3"></a><h4>3.8.3 Send/Receive counts and datatypes</h4>
<p>In many collective operations, MPI allows for different send/receive counts
and types, as long as sendcount*sizeof(sendtype) == recvcount*sizeof(recvtype).
NCCL does not allow that, defining a single count and a single datatype.
<p>For AllGather and ReduceScatter operations, the count is equal to the
per-rank size, which is the smallest size ; the other count being equal to
<i>nranks*count</i>. The function prototype clearly shows which count is
provided, i.e. <i>sendcount</i> for ncclAllgather and <i>recvcount</i> for
ncclReduceScatter.
<p>Remark : when performing or comparing AllReduce operations using a
combination of ReduceScatter and AllGather, users should therefore define the
sendcount and recvcount as the total count divided by the number of ranks, with
the correct rounding (up) if it is not a perfect multiple of the number of
ranks.
<a name="#3.8.4"></a><h4>3.8.4 In place semantics</h4>
<p>Contrary to MPI, NCCL does not define a special "in-place" value to
replace pointers. Instead, NCCL optimizes the case where the provided pointers
are effectively "in place".
<p>For ncclReduce and ncclAllreduce functions, this simply means that passing <i>sendBuff
== recvBuff</i> will perform in place operations, storing final results at the
same place as initial data was read from.
<p>For ncclReduceScatter and ncclAllGather, in place operations will be done
when the per-rank pointer is located at the rank offset of the global buffer.
More precisely, these calls will be considered in place :
<pre>
  ncclReduceScatter(data, data+rank*recvcount, recvcount, datatype, op, comm, stream);
  ncclAllGather(data+rank*sendcount, data, sendcount, datatype, op, comm, stream);
</pre>
<a name="#3"></a><h2>3 API</h2>
<a name="#4.1"></a><h3>4.1 Initialization</h3>
<a name="#4.2.1"></a><h4>4.2.1 ncclGetUniqueId</h4>
<a name="#4.2.2"></a><h4>4.2.2 ncclCommInitRank</h4>
<a name="#4.2.3"></a><h4>4.2.3 ncclCommInitAll</h4>
<a name="#4.2"></a><h3>4.2 Collective operations</h3>
<a name="#4.3.1"></a><h4>4.3.1 ncclAllReduce</h4>
<a name="#4.3.2"></a><h4>4.3.2 ncclReduce</h4>
<a name="#4.3.3"></a><h4>4.3.3 ncclBcast</h4>
<a name="#4.3.4"></a><h4>4.3.4 ncclReduceScatter</h4>
<a name="#4.3.5"></a><h4>4.3.5 ncclAllGather</h4>
<a name="#4.3"></a><h3>4.3 Group calls</h3>
<a name="#4.4.1"></a><h4>4.4.1 ncclGroupStart</h4>
<a name="#4.4.2"></a><h4>4.4.2 ncclGroupEnd</h4>
</div>
<div class="menu">
<p><a href="#top">Top</a>
<p><b>Table of contents</b>
$CONTENTS
</div>
</body>
</html>
